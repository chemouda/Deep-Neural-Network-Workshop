1. Neural Networks and Deep Learning
2. How DNN may be applied to other problems in computer vision, and also in speech, natural language processing, and other domains
3. perceptron - inspired by earlier work by Warren McCulloch and Walter Pitts.
4. Modern NN- sigmoid neuron. 
5. how do perceptrons work?
- ex. NAND gates are universal for computation, it follows that perceptrons are also universal for computation.
- The computational universality of perceptrons is simultaneously reassuring and disappointing. It's reassuring because it tells us that networks of perceptrons can be as powerful as any other computing device. But it's also disappointing, because it makes it seem as though perceptrons are merely a new type of NAND gate. That's hardly big news!
- Neural networks can simply learn to solve problems, sometimes problems where it would be extremely difficult to directly design a conventional circuit.
6. Why Sigmoid Neuron network people uses instead of Perceptron ?
7. How should we interpret the output from a sigmoid neuron ?
8. Multilayer perceptrons or MLPs 
9. feedforward neural networks vs Recurrent Neural Networks
10. How to use correct input neuron, output neuron, number of hidden layers based on problem statement !
11. Is there some heuristic that would tell us in advance that we should use the N-output encoding instead of the N-output encoding?
12. Now no assumption , We want fact ! It does this by weighing up evidence from the hidden layer of neurons. What are those hidden neurons doing ?
13. Importance of vector
14. Math behind Neural Network Learning
15. How it is easy to program Algebraic form of Sigmoid or any other NN Fn ?
16. How to minimize the Cost C(w,b) weights and biases with Gradient Descent. The way gradient descent works ? variations of gradient descent ?
17. How to develop good neural network architecture ! How to tune hyper-parameters such as learning rate, hidden layers ?
18. Why introduce the quadratic cost? 
19. Using calculus to minimize that just won't work!
- Okay, so calculus doesn't work. Fortunately, there is a beautiful analogy which suggests an algorithm which works pretty well. 
20. How stochastic gradient descent can be used to speed up learning ?
21. What is incremental learning ? incremental learning vs stochastic gradient descent
