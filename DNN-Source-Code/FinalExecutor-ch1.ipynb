{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = network.Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll use stochastic gradient descent to learn from the MNIST training_data over 30 epochs, with a mini-batch size of 10, and a learning rate of η=3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9081 / 10000\n",
      "Epoch 1: 9316 / 10000\n",
      "Epoch 2: 9290 / 10000\n",
      "Epoch 3: 9389 / 10000\n",
      "Epoch 4: 9344 / 10000\n",
      "Epoch 5: 9433 / 10000\n",
      "Epoch 6: 9434 / 10000\n",
      "Epoch 7: 9439 / 10000\n",
      "Epoch 8: 9484 / 10000\n",
      "Epoch 9: 9481 / 10000\n",
      "Epoch 10: 9510 / 10000\n",
      "Epoch 11: 9500 / 10000\n",
      "Epoch 12: 9490 / 10000\n",
      "Epoch 13: 9490 / 10000\n",
      "Epoch 14: 9521 / 10000\n",
      "Epoch 15: 9522 / 10000\n",
      "Epoch 16: 9498 / 10000\n",
      "Epoch 17: 9507 / 10000\n",
      "Epoch 18: 9527 / 10000\n",
      "Epoch 19: 9523 / 10000\n",
      "Epoch 20: 9522 / 10000\n",
      "Epoch 21: 9465 / 10000\n",
      "Epoch 22: 9534 / 10000\n",
      "Epoch 23: 9524 / 10000\n",
      "Epoch 24: 9525 / 10000\n",
      "Epoch 25: 9515 / 10000\n",
      "Epoch 26: 9538 / 10000\n",
      "Epoch 27: 9515 / 10000\n",
      "Epoch 28: 9523 / 10000\n",
      "Epoch 29: 9551 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10, 3.0, test_data = test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rerun the above experiment, changing the number of hidden neurons to 100, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 7445 / 10000\n",
      "Epoch 1: 7587 / 10000\n",
      "Epoch 2: 7629 / 10000\n",
      "Epoch 3: 7663 / 10000\n",
      "Epoch 4: 7722 / 10000\n",
      "Epoch 5: 7810 / 10000\n",
      "Epoch 6: 8605 / 10000\n",
      "Epoch 7: 8602 / 10000\n",
      "Epoch 8: 8640 / 10000\n",
      "Epoch 9: 8675 / 10000\n",
      "Epoch 10: 8679 / 10000\n",
      "Epoch 11: 8691 / 10000\n",
      "Epoch 12: 8687 / 10000\n",
      "Epoch 13: 8698 / 10000\n",
      "Epoch 14: 8741 / 10000\n",
      "Epoch 15: 9592 / 10000\n",
      "Epoch 16: 9590 / 10000\n",
      "Epoch 17: 9592 / 10000\n",
      "Epoch 18: 9612 / 10000\n",
      "Epoch 19: 9603 / 10000\n",
      "Epoch 20: 9620 / 10000\n",
      "Epoch 21: 9624 / 10000\n",
      "Epoch 22: 9633 / 10000\n",
      "Epoch 23: 9632 / 10000\n",
      "Epoch 24: 9642 / 10000\n",
      "Epoch 25: 9655 / 10000\n",
      "Epoch 26: 9647 / 10000\n",
      "Epoch 27: 9653 / 10000\n",
      "Epoch 28: 9650 / 10000\n",
      "Epoch 29: 9657 / 10000\n"
     ]
    }
   ],
   "source": [
    "net = network.Network([784, 100, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least in this case, using more hidden neurons helps us get better results\n",
    "Of course, to obtain these accuracies I had to make specific choices for the number of epochs of training, the mini-batch size, and the learning rate, ηη. As I mentioned above, these are known as hyper-parameters for our neural network, in order to distinguish them from the parameters (weights and biases) learnt by our learning algorithm. If we choose our hyper-parameters poorly, we can get bad results. Suppose, for example, that we'd chosen the learning rate to be η=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9552 / 10000\n",
      "Epoch 1: 9553 / 10000\n",
      "Epoch 2: 9554 / 10000\n",
      "Epoch 3: 9553 / 10000\n",
      "Epoch 4: 9552 / 10000\n",
      "Epoch 5: 9552 / 10000\n",
      "Epoch 6: 9552 / 10000\n",
      "Epoch 7: 9552 / 10000\n",
      "Epoch 8: 9548 / 10000\n",
      "Epoch 9: 9549 / 10000\n",
      "Epoch 10: 9548 / 10000\n",
      "Epoch 11: 9548 / 10000\n",
      "Epoch 12: 9547 / 10000\n",
      "Epoch 13: 9549 / 10000\n",
      "Epoch 14: 9546 / 10000\n",
      "Epoch 15: 9546 / 10000\n",
      "Epoch 16: 9544 / 10000\n",
      "Epoch 17: 9546 / 10000\n",
      "Epoch 18: 9545 / 10000\n",
      "Epoch 19: 9547 / 10000\n",
      "Epoch 20: 9546 / 10000\n",
      "Epoch 21: 9547 / 10000\n",
      "Epoch 22: 9547 / 10000\n",
      "Epoch 23: 9546 / 10000\n",
      "Epoch 24: 9543 / 10000\n",
      "Epoch 25: 9543 / 10000\n",
      "Epoch 26: 9544 / 10000\n",
      "Epoch 27: 9543 / 10000\n",
      "Epoch 28: 9543 / 10000\n",
      "Epoch 29: 9543 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10, 0.001, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you can see that the performance of the network is getting slowly better over time. That suggests increasing the learning rate, say to η=0.01. If we do that, we get better results, which suggests increasing the learning rate again. (If making a change improves things, try doing more!) If we do that several times over, we'll end up with a learning rate of something like η=1.0 (and perhaps fine tune to 3.0), which is close to our earlier experiments. So even though we initially made a poor choice of hyper-parameters, we at least got enough information to help us improve our choice of hyper-parameters.\n",
    "In general, debugging a neural network can be challenging. This is especially true when the initial choice of hyper-parameters produces results no better than random noise. Suppose we try the successful 30 hidden neuron network architecture from earlier, but with the learning rate changed to η=100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1020 / 10000\n",
      "Epoch 1: 1008 / 10000\n",
      "Epoch 2: 1008 / 10000\n",
      "Epoch 3: 1008 / 10000\n",
      "Epoch 4: 1008 / 10000\n",
      "Epoch 5: 1008 / 10000\n",
      "Epoch 6: 1008 / 10000\n",
      "Epoch 7: 1008 / 10000\n",
      "Epoch 8: 1008 / 10000\n",
      "Epoch 9: 1008 / 10000\n",
      "Epoch 10: 1008 / 10000\n",
      "Epoch 11: 1008 / 10000\n",
      "Epoch 12: 1008 / 10000\n",
      "Epoch 13: 1008 / 10000\n",
      "Epoch 14: 1008 / 10000\n",
      "Epoch 15: 1008 / 10000\n",
      "Epoch 16: 1008 / 10000\n",
      "Epoch 17: 1009 / 10000\n",
      "Epoch 18: 1009 / 10000\n",
      "Epoch 19: 1009 / 10000\n",
      "Epoch 20: 1009 / 10000\n",
      "Epoch 21: 1009 / 10000\n",
      "Epoch 22: 1009 / 10000\n",
      "Epoch 23: 1009 / 10000\n",
      "Epoch 24: 1009 / 10000\n",
      "Epoch 25: 1009 / 10000\n",
      "Epoch 26: 1009 / 10000\n",
      "Epoch 27: 1009 / 10000\n",
      "Epoch 28: 1009 / 10000\n",
      "Epoch 29: 1009 / 10000\n"
     ]
    }
   ],
   "source": [
    "net = network.Network([784, 30, 10])\n",
    "net.SGD(training_data, 30, 10, 100.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we know from our earlier experiments that the right thing to do is to decrease the learning rate. But if we were coming to this problem for the first time then there wouldn't be much in the output to guide us on what to do. We might worry not only about the learning rate, but about every other aspect of our neural network. We might wonder if we've initialized the weights and biases in a way that makes it hard for the network to learn? Or maybe we don't have enough training data to get meaningful learning?\n",
    "Perhaps we haven't run for enough epochs? Or maybe it's impossible for a neural network with this architecture to learn to recognize handwritten digits? Maybe the learning rate is too low? Or, maybe, the learning rate is too high? When you're coming to a problem for the first time, you're not always sure.\n",
    "debugging a neural network is not trivial, and, just as for ordinary programming, there is an art to it. More generally, we need to develop heuristics for choosing good hyper-parameters and a good architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
