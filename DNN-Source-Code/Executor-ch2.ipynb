{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-entropy is easy to implement as part of a program which learns using gradient descent and backpropagation.\n",
    "The new program is called network2.py, and incorporates not just the cross-entropy, but also several other techniques developed.\n",
    "For now, let's look at how well our new program classifies MNIST digits. As was the case in Chapter 1, we'll use a network with 3030 hidden neurons, and we'll use a mini-batch size of 1010. We set the learning rate to η=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 1 we used the quadratic cost and a learning rate of η=3.0η=3.0. As discussed above, it's not possible to say precisely what it means to use the \"same\" learning rate when the cost function is changed. For both cost functions I experimented to find a learning rate that provides near-optimal performance, given the other hyper-parameter choices. \n",
    "\n",
    "There is, incidentally, a very rough general heuristic for relating the learning rate for the cross-entropy and the quadratic cost. As we saw earlier, the gradient terms for the quadratic cost have an extra σ′=σ(1−σ)σ′=σ(1−σ) term in them. Suppose we average this over values for σσ, ∫10dσσ(1−σ)=1/6∫01dσσ(1−σ)=1/6. We see that (very roughly) the quadratic cost learns an average of 66 times slower, for the same learning rate. This suggests that a reasonable starting point is to divide the learning rate for the quadratic cost by 66. Of course, this argument is far from rigorous, and shouldn't be taken too seriously. Still, it can sometimes be a useful starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9142 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9304 / 10000\n",
      "\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9361 / 10000\n",
      "\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9383 / 10000\n",
      "\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9404 / 10000\n",
      "\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9416 / 10000\n",
      "\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9419 / 10000\n",
      "\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9481 / 10000\n",
      "\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9497 / 10000\n",
      "\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9487 / 10000\n",
      "\n",
      "Epoch 10 training complete\n",
      "Accuracy on evaluation data: 9494 / 10000\n",
      "\n",
      "Epoch 11 training complete\n",
      "Accuracy on evaluation data: 9519 / 10000\n",
      "\n",
      "Epoch 12 training complete\n",
      "Accuracy on evaluation data: 9514 / 10000\n",
      "\n",
      "Epoch 13 training complete\n",
      "Accuracy on evaluation data: 9508 / 10000\n",
      "\n",
      "Epoch 14 training complete\n",
      "Accuracy on evaluation data: 9519 / 10000\n",
      "\n",
      "Epoch 15 training complete\n",
      "Accuracy on evaluation data: 9521 / 10000\n",
      "\n",
      "Epoch 16 training complete\n",
      "Accuracy on evaluation data: 9511 / 10000\n",
      "\n",
      "Epoch 17 training complete\n",
      "Accuracy on evaluation data: 9537 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Accuracy on evaluation data: 9515 / 10000\n",
      "\n",
      "Epoch 19 training complete\n",
      "Accuracy on evaluation data: 9514 / 10000\n",
      "\n",
      "Epoch 20 training complete\n",
      "Accuracy on evaluation data: 9523 / 10000\n",
      "\n",
      "Epoch 21 training complete\n",
      "Accuracy on evaluation data: 9532 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Accuracy on evaluation data: 9527 / 10000\n",
      "\n",
      "Epoch 23 training complete\n",
      "Accuracy on evaluation data: 9496 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Accuracy on evaluation data: 9524 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Accuracy on evaluation data: 9506 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Accuracy on evaluation data: 9532 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Accuracy on evaluation data: 9528 / 10000\n",
      "\n",
      "Epoch 28 training complete\n",
      "Accuracy on evaluation data: 9511 / 10000\n",
      "\n",
      "Epoch 29 training complete\n",
      "Accuracy on evaluation data: 9526 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [9142,\n",
       "  9304,\n",
       "  9361,\n",
       "  9383,\n",
       "  9404,\n",
       "  9416,\n",
       "  9419,\n",
       "  9481,\n",
       "  9497,\n",
       "  9487,\n",
       "  9494,\n",
       "  9519,\n",
       "  9514,\n",
       "  9508,\n",
       "  9519,\n",
       "  9521,\n",
       "  9511,\n",
       "  9537,\n",
       "  9515,\n",
       "  9514,\n",
       "  9523,\n",
       "  9532,\n",
       "  9527,\n",
       "  9496,\n",
       "  9524,\n",
       "  9506,\n",
       "  9532,\n",
       "  9528,\n",
       "  9511,\n",
       "  9526],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import network2\n",
    "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data, 30, 10, 0.5, evaluation_data=test_data,monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, by the way, that the net.large_weight_initializer() command is used to initialize the weights and biases in the same way as described in Chapter 1. We need to run this command because later in this chapter we'll change the default weight initialization in our networks. The result from running the above sequence of commands is a network with 95.4995.49 percent accuracy. This is pretty close to the result we obtained in Chapter 1, 95.4295.42 percent, using the quadratic cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look also at the case where we use 100100 hidden neurons, the cross-entropy, and otherwise keep the parameters the same. In this case we obtain an accuracy of 96.8296.82 percent. That's a substantial improvement over the results from Chapter 1, where we obtained a classification accuracy of 96.5996.59 percent, using the quadratic cost. That may look like a small change, but consider that the error rate has dropped from 3.413.41 percent to 3.183.18 percent. That is, we've eliminated about one in fourteen of the original errors. That's quite a handy improvement.\n",
    "\n",
    "It's encouraging that the cross-entropy cost gives us similar or better results than the quadratic cost. However, these results don't conclusively prove that the cross-entropy is a better choice. The reason is that I've put only a little effort into choosing hyper-parameters such as learning rate, mini-batch size, and so on. For the improvement to be really convincing we'd need to do a thorough job optimizing such hyper-parameters. Still, the results are encouraging, and reinforce our earlier theoretical argument that the cross-entropy is a better choice than the quadratic cost.\n",
    "\n",
    "This, by the way, is part of a general pattern that we'll see through this chapter and, indeed, through much of the rest of the book. We'll develop a new technique, we'll try it out, and we'll get \"improved\" results. It is, of course, nice that we see such improvements. But the interpretation of such improvements is always problematic. They're only truly convincing if we see an improvement after putting tremendous effort into optimizing all the other hyper-parameters. That's a great deal of work, requiring lots of computing power, and we're not usually going to do such an exhaustive investigation. Instead, we'll proceed on the basis of informal tests like those done above. Still, you should keep in mind that such tests fall short of definitive proof, and remain alert to signs that the arguments are breaking down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.large_weight_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model got overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 1.84139565543\n",
      "Accuracy on evaluation data: 6202 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.38580417171\n",
      "Accuracy on evaluation data: 6893 / 10000\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.15467253384\n",
      "Accuracy on evaluation data: 7166 / 10000\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 0.919018693931\n",
      "Accuracy on evaluation data: 7723 / 10000\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 0.817511801552\n",
      "Accuracy on evaluation data: 7778 / 10000\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 0.67193699903\n",
      "Accuracy on evaluation data: 7889 / 10000\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 0.553130571116\n",
      "Accuracy on evaluation data: 8023 / 10000\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 0.507954768934\n",
      "Accuracy on evaluation data: 8049 / 10000\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 0.437310636767\n",
      "Accuracy on evaluation data: 8181 / 10000\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.411839736111\n",
      "Accuracy on evaluation data: 8105 / 10000\n",
      "\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 0.356936069801\n",
      "Accuracy on evaluation data: 8208 / 10000\n",
      "\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 0.311784413584\n",
      "Accuracy on evaluation data: 8248 / 10000\n",
      "\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 0.28114843984\n",
      "Accuracy on evaluation data: 8266 / 10000\n",
      "\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 0.264876472722\n",
      "Accuracy on evaluation data: 8292 / 10000\n",
      "\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 0.233728280325\n",
      "Accuracy on evaluation data: 8291 / 10000\n",
      "\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 0.217035143398\n",
      "Accuracy on evaluation data: 8330 / 10000\n",
      "\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 0.202224801976\n",
      "Accuracy on evaluation data: 8320 / 10000\n",
      "\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 0.180623828461\n",
      "Accuracy on evaluation data: 8348 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 0.169336166016\n",
      "Accuracy on evaluation data: 8364 / 10000\n",
      "\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 0.16502900685\n",
      "Accuracy on evaluation data: 8355 / 10000\n",
      "\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 0.157584371769\n",
      "Accuracy on evaluation data: 8340 / 10000\n",
      "\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 0.15260889594\n",
      "Accuracy on evaluation data: 8317 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 0.133108166153\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 0.124200845689\n",
      "Accuracy on evaluation data: 8388 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 0.118614321933\n",
      "Accuracy on evaluation data: 8368 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 0.112002508967\n",
      "Accuracy on evaluation data: 8378 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 0.106131585091\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 0.10312668343\n",
      "Accuracy on evaluation data: 8378 / 10000\n",
      "\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 0.0945753907908\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 0.0909385064233\n",
      "Accuracy on evaluation data: 8416 / 10000\n",
      "\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 0.0871585181548\n",
      "Accuracy on evaluation data: 8428 / 10000\n",
      "\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 0.0839216265242\n",
      "Accuracy on evaluation data: 8403 / 10000\n",
      "\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 0.0795603610547\n",
      "Accuracy on evaluation data: 8434 / 10000\n",
      "\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 0.0765736083505\n",
      "Accuracy on evaluation data: 8421 / 10000\n",
      "\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.0743902331753\n",
      "Accuracy on evaluation data: 8411 / 10000\n",
      "\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.0702205217191\n",
      "Accuracy on evaluation data: 8429 / 10000\n",
      "\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.0674320736584\n",
      "Accuracy on evaluation data: 8440 / 10000\n",
      "\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.0651351480201\n",
      "Accuracy on evaluation data: 8436 / 10000\n",
      "\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.0629637802932\n",
      "Accuracy on evaluation data: 8432 / 10000\n",
      "\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.0604911762004\n",
      "Accuracy on evaluation data: 8434 / 10000\n",
      "\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.0590167231626\n",
      "Accuracy on evaluation data: 8429 / 10000\n",
      "\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.0563555171956\n",
      "Accuracy on evaluation data: 8426 / 10000\n",
      "\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.0542929020111\n",
      "Accuracy on evaluation data: 8430 / 10000\n",
      "\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.0519875665967\n",
      "Accuracy on evaluation data: 8439 / 10000\n",
      "\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.0499889020788\n",
      "Accuracy on evaluation data: 8439 / 10000\n",
      "\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 0.0489614017732\n",
      "Accuracy on evaluation data: 8439 / 10000\n",
      "\n",
      "Epoch 46 training complete\n",
      "Cost on training data: 0.0473417729851\n",
      "Accuracy on evaluation data: 8460 / 10000\n",
      "\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.045552813721\n",
      "Accuracy on evaluation data: 8447 / 10000\n",
      "\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.0448091602191\n",
      "Accuracy on evaluation data: 8444 / 10000\n",
      "\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.0433551198074\n",
      "Accuracy on evaluation data: 8438 / 10000\n",
      "\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.0420914526557\n",
      "Accuracy on evaluation data: 8450 / 10000\n",
      "\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.0411970519994\n",
      "Accuracy on evaluation data: 8436 / 10000\n",
      "\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.0397411678493\n",
      "Accuracy on evaluation data: 8431 / 10000\n",
      "\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.0388211527713\n",
      "Accuracy on evaluation data: 8444 / 10000\n",
      "\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.0380648058793\n",
      "Accuracy on evaluation data: 8441 / 10000\n",
      "\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.0368006898395\n",
      "Accuracy on evaluation data: 8443 / 10000\n",
      "\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.0358058021437\n",
      "Accuracy on evaluation data: 8448 / 10000\n",
      "\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.0351168909757\n",
      "Accuracy on evaluation data: 8449 / 10000\n",
      "\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.0341694147747\n",
      "Accuracy on evaluation data: 8448 / 10000\n",
      "\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.033315995153\n",
      "Accuracy on evaluation data: 8449 / 10000\n",
      "\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.0325408401988\n",
      "Accuracy on evaluation data: 8448 / 10000\n",
      "\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.0317328640254\n",
      "Accuracy on evaluation data: 8447 / 10000\n",
      "\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.0310883823547\n",
      "Accuracy on evaluation data: 8445 / 10000\n",
      "\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.0302506745564\n",
      "Accuracy on evaluation data: 8443 / 10000\n",
      "\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.0296885892421\n",
      "Accuracy on evaluation data: 8448 / 10000\n",
      "\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.0290631000128\n",
      "Accuracy on evaluation data: 8444 / 10000\n",
      "\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 0.0284640117717\n",
      "Accuracy on evaluation data: 8446 / 10000\n",
      "\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.0278360262543\n",
      "Accuracy on evaluation data: 8446 / 10000\n",
      "\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.0274511459648\n",
      "Accuracy on evaluation data: 8432 / 10000\n",
      "\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.0267973119225\n",
      "Accuracy on evaluation data: 8452 / 10000\n",
      "\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.0264675797255\n",
      "Accuracy on evaluation data: 8445 / 10000\n",
      "\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.0258399154828\n",
      "Accuracy on evaluation data: 8444 / 10000\n",
      "\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.0253346154155\n",
      "Accuracy on evaluation data: 8442 / 10000\n",
      "\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.0248791616508\n",
      "Accuracy on evaluation data: 8447 / 10000\n",
      "\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.0243938584345\n",
      "Accuracy on evaluation data: 8444 / 10000\n",
      "\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.0240696465462\n",
      "Accuracy on evaluation data: 8454 / 10000\n",
      "\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 0.023540513559\n",
      "Accuracy on evaluation data: 8448 / 10000\n",
      "\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.0231822842723\n",
      "Accuracy on evaluation data: 8456 / 10000\n",
      "\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.0227039993383\n",
      "Accuracy on evaluation data: 8453 / 10000\n",
      "\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.0223495173034\n",
      "Accuracy on evaluation data: 8449 / 10000\n",
      "\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 0.0219040716713\n",
      "Accuracy on evaluation data: 8444 / 10000\n",
      "\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 0.0215157074689\n",
      "Accuracy on evaluation data: 8449 / 10000\n",
      "\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 0.021115121192\n",
      "Accuracy on evaluation data: 8446 / 10000\n",
      "\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 0.0207295865991\n",
      "Accuracy on evaluation data: 8444 / 10000\n",
      "\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 0.0203602263554\n",
      "Accuracy on evaluation data: 8461 / 10000\n",
      "\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 0.0198892317572\n",
      "Accuracy on evaluation data: 8453 / 10000\n",
      "\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 0.0194801460439\n",
      "Accuracy on evaluation data: 8456 / 10000\n",
      "\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 0.0191154550581\n",
      "Accuracy on evaluation data: 8447 / 10000\n",
      "\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 0.0187648890803\n",
      "Accuracy on evaluation data: 8452 / 10000\n",
      "\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 0.0184819453592\n",
      "Accuracy on evaluation data: 8455 / 10000\n",
      "\n",
      "Epoch 90 training complete\n",
      "Cost on training data: 0.0181419882638\n",
      "Accuracy on evaluation data: 8445 / 10000\n",
      "\n",
      "Epoch 91 training complete\n",
      "Cost on training data: 0.0178486166089\n",
      "Accuracy on evaluation data: 8455 / 10000\n",
      "\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 0.0175904229336\n",
      "Accuracy on evaluation data: 8460 / 10000\n",
      "\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 0.0173352307254\n",
      "Accuracy on evaluation data: 8453 / 10000\n",
      "\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 0.017056027492\n",
      "Accuracy on evaluation data: 8463 / 10000\n",
      "\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 0.0168435294612\n",
      "Accuracy on evaluation data: 8460 / 10000\n",
      "\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 0.0165726342569\n",
      "Accuracy on evaluation data: 8462 / 10000\n",
      "\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 0.0163424104759\n",
      "Accuracy on evaluation data: 8463 / 10000\n",
      "\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 0.016148608186\n",
      "Accuracy on evaluation data: 8461 / 10000\n",
      "\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 0.0159190250493\n",
      "Accuracy on evaluation data: 8468 / 10000\n",
      "\n",
      "Epoch 100 training complete\n",
      "Cost on training data: 0.0157109261238\n",
      "Accuracy on evaluation data: 8468 / 10000\n",
      "\n",
      "Epoch 101 training complete\n",
      "Cost on training data: 0.015504492642\n",
      "Accuracy on evaluation data: 8467 / 10000\n",
      "\n",
      "Epoch 102 training complete\n",
      "Cost on training data: 0.0153199363161\n",
      "Accuracy on evaluation data: 8471 / 10000\n",
      "\n",
      "Epoch 103 training complete\n",
      "Cost on training data: 0.0151271531164\n",
      "Accuracy on evaluation data: 8471 / 10000\n",
      "\n",
      "Epoch 104 training complete\n",
      "Cost on training data: 0.0149494648241\n",
      "Accuracy on evaluation data: 8469 / 10000\n",
      "\n",
      "Epoch 105 training complete\n",
      "Cost on training data: 0.0147541198854\n",
      "Accuracy on evaluation data: 8470 / 10000\n",
      "\n",
      "Epoch 106 training complete\n",
      "Cost on training data: 0.0145815160313\n",
      "Accuracy on evaluation data: 8472 / 10000\n",
      "\n",
      "Epoch 107 training complete\n",
      "Cost on training data: 0.0144183939045\n",
      "Accuracy on evaluation data: 8472 / 10000\n",
      "\n",
      "Epoch 108 training complete\n",
      "Cost on training data: 0.0142421268329\n",
      "Accuracy on evaluation data: 8477 / 10000\n",
      "\n",
      "Epoch 109 training complete\n",
      "Cost on training data: 0.0140966344253\n",
      "Accuracy on evaluation data: 8470 / 10000\n",
      "\n",
      "Epoch 110 training complete\n",
      "Cost on training data: 0.0139172525644\n",
      "Accuracy on evaluation data: 8470 / 10000\n",
      "\n",
      "Epoch 111 training complete\n",
      "Cost on training data: 0.01375648348\n",
      "Accuracy on evaluation data: 8475 / 10000\n",
      "\n",
      "Epoch 112 training complete\n",
      "Cost on training data: 0.0135986097776\n",
      "Accuracy on evaluation data: 8473 / 10000\n",
      "\n",
      "Epoch 113 training complete\n",
      "Cost on training data: 0.0134509178381\n",
      "Accuracy on evaluation data: 8476 / 10000\n",
      "\n",
      "Epoch 114 training complete\n",
      "Cost on training data: 0.0133001534912\n",
      "Accuracy on evaluation data: 8474 / 10000\n",
      "\n",
      "Epoch 115 training complete\n",
      "Cost on training data: 0.0131688703621\n",
      "Accuracy on evaluation data: 8476 / 10000\n",
      "\n",
      "Epoch 116 training complete\n",
      "Cost on training data: 0.0130139309597\n",
      "Accuracy on evaluation data: 8481 / 10000\n",
      "\n",
      "Epoch 117 training complete\n",
      "Cost on training data: 0.0128775051084\n",
      "Accuracy on evaluation data: 8481 / 10000\n",
      "\n",
      "Epoch 118 training complete\n",
      "Cost on training data: 0.0127338437292\n",
      "Accuracy on evaluation data: 8478 / 10000\n",
      "\n",
      "Epoch 119 training complete\n",
      "Cost on training data: 0.0126167889236\n",
      "Accuracy on evaluation data: 8477 / 10000\n",
      "\n",
      "Epoch 120 training complete\n",
      "Cost on training data: 0.0124665189858\n",
      "Accuracy on evaluation data: 8477 / 10000\n",
      "\n",
      "Epoch 121 training complete\n",
      "Cost on training data: 0.0123384997607\n",
      "Accuracy on evaluation data: 8480 / 10000\n",
      "\n",
      "Epoch 122 training complete\n",
      "Cost on training data: 0.012217740323\n",
      "Accuracy on evaluation data: 8482 / 10000\n",
      "\n",
      "Epoch 123 training complete\n",
      "Cost on training data: 0.012082568994\n",
      "Accuracy on evaluation data: 8479 / 10000\n",
      "\n",
      "Epoch 124 training complete\n",
      "Cost on training data: 0.0119557880555\n",
      "Accuracy on evaluation data: 8478 / 10000\n",
      "\n",
      "Epoch 125 training complete\n",
      "Cost on training data: 0.011837555244\n",
      "Accuracy on evaluation data: 8482 / 10000\n",
      "\n",
      "Epoch 126 training complete\n",
      "Cost on training data: 0.0117157128661\n",
      "Accuracy on evaluation data: 8479 / 10000\n",
      "\n",
      "Epoch 127 training complete\n",
      "Cost on training data: 0.0116116987724\n",
      "Accuracy on evaluation data: 8480 / 10000\n",
      "\n",
      "Epoch 128 training complete\n",
      "Cost on training data: 0.0114812921155\n",
      "Accuracy on evaluation data: 8474 / 10000\n",
      "\n",
      "Epoch 129 training complete\n",
      "Cost on training data: 0.0113673046535\n",
      "Accuracy on evaluation data: 8479 / 10000\n",
      "\n",
      "Epoch 130 training complete\n",
      "Cost on training data: 0.0112610721785\n",
      "Accuracy on evaluation data: 8481 / 10000\n",
      "\n",
      "Epoch 131 training complete\n",
      "Cost on training data: 0.0111494949425\n",
      "Accuracy on evaluation data: 8479 / 10000\n",
      "\n",
      "Epoch 132 training complete\n",
      "Cost on training data: 0.0110436005961\n",
      "Accuracy on evaluation data: 8481 / 10000\n",
      "\n",
      "Epoch 133 training complete\n",
      "Cost on training data: 0.0109419759325\n",
      "Accuracy on evaluation data: 8481 / 10000\n",
      "\n",
      "Epoch 134 training complete\n",
      "Cost on training data: 0.0108402942962\n",
      "Accuracy on evaluation data: 8483 / 10000\n",
      "\n",
      "Epoch 135 training complete\n",
      "Cost on training data: 0.0107431858864\n",
      "Accuracy on evaluation data: 8479 / 10000\n",
      "\n",
      "Epoch 136 training complete\n",
      "Cost on training data: 0.01063955584\n",
      "Accuracy on evaluation data: 8482 / 10000\n",
      "\n",
      "Epoch 137 training complete\n",
      "Cost on training data: 0.0105458309091\n",
      "Accuracy on evaluation data: 8483 / 10000\n",
      "\n",
      "Epoch 138 training complete\n",
      "Cost on training data: 0.01045262783\n",
      "Accuracy on evaluation data: 8480 / 10000\n",
      "\n",
      "Epoch 139 training complete\n",
      "Cost on training data: 0.0103589914228\n",
      "Accuracy on evaluation data: 8481 / 10000\n",
      "\n",
      "Epoch 140 training complete\n",
      "Cost on training data: 0.0102686318222\n",
      "Accuracy on evaluation data: 8480 / 10000\n",
      "\n",
      "Epoch 141 training complete\n",
      "Cost on training data: 0.0101813473215\n",
      "Accuracy on evaluation data: 8486 / 10000\n",
      "\n",
      "Epoch 142 training complete\n",
      "Cost on training data: 0.0100950712038\n",
      "Accuracy on evaluation data: 8484 / 10000\n",
      "\n",
      "Epoch 143 training complete\n",
      "Cost on training data: 0.0100097349939\n",
      "Accuracy on evaluation data: 8483 / 10000\n",
      "\n",
      "Epoch 144 training complete\n",
      "Cost on training data: 0.00992504280134\n",
      "Accuracy on evaluation data: 8482 / 10000\n",
      "\n",
      "Epoch 145 training complete\n",
      "Cost on training data: 0.00984320942345\n",
      "Accuracy on evaluation data: 8480 / 10000\n",
      "\n",
      "Epoch 146 training complete\n",
      "Cost on training data: 0.00976416946874\n",
      "Accuracy on evaluation data: 8481 / 10000\n",
      "\n",
      "Epoch 147 training complete\n",
      "Cost on training data: 0.00968258480848\n",
      "Accuracy on evaluation data: 8482 / 10000\n",
      "\n",
      "Epoch 148 training complete\n",
      "Cost on training data: 0.00960324753561\n",
      "Accuracy on evaluation data: 8482 / 10000\n",
      "\n",
      "Epoch 149 training complete\n",
      "Cost on training data: 0.00952678504223\n",
      "Accuracy on evaluation data: 8482 / 10000\n",
      "\n",
      "Epoch 150 training complete\n",
      "Cost on training data: 0.00944939859157\n",
      "Accuracy on evaluation data: 8483 / 10000\n",
      "\n",
      "Epoch 151 training complete\n",
      "Cost on training data: 0.00937745123942\n",
      "Accuracy on evaluation data: 8487 / 10000\n",
      "\n",
      "Epoch 152 training complete\n",
      "Cost on training data: 0.00930256520237\n",
      "Accuracy on evaluation data: 8485 / 10000\n",
      "\n",
      "Epoch 153 training complete\n",
      "Cost on training data: 0.00923029357752\n",
      "Accuracy on evaluation data: 8481 / 10000\n",
      "\n",
      "Epoch 154 training complete\n",
      "Cost on training data: 0.00915967851478\n",
      "Accuracy on evaluation data: 8484 / 10000\n",
      "\n",
      "Epoch 155 training complete\n",
      "Cost on training data: 0.00909167449084\n",
      "Accuracy on evaluation data: 8484 / 10000\n",
      "\n",
      "Epoch 156 training complete\n",
      "Cost on training data: 0.0090220589728\n",
      "Accuracy on evaluation data: 8486 / 10000\n",
      "\n",
      "Epoch 157 training complete\n",
      "Cost on training data: 0.00895222779629\n",
      "Accuracy on evaluation data: 8488 / 10000\n",
      "\n",
      "Epoch 158 training complete\n",
      "Cost on training data: 0.00888693447945\n",
      "Accuracy on evaluation data: 8486 / 10000\n",
      "\n",
      "Epoch 159 training complete\n",
      "Cost on training data: 0.00881925491576\n",
      "Accuracy on evaluation data: 8487 / 10000\n",
      "\n",
      "Epoch 160 training complete\n",
      "Cost on training data: 0.00875521232118\n",
      "Accuracy on evaluation data: 8487 / 10000\n",
      "\n",
      "Epoch 161 training complete\n",
      "Cost on training data: 0.00869234275672\n",
      "Accuracy on evaluation data: 8487 / 10000\n",
      "\n",
      "Epoch 162 training complete\n",
      "Cost on training data: 0.00862719193484\n",
      "Accuracy on evaluation data: 8487 / 10000\n",
      "\n",
      "Epoch 163 training complete\n",
      "Cost on training data: 0.00856396616926\n",
      "Accuracy on evaluation data: 8489 / 10000\n",
      "\n",
      "Epoch 164 training complete\n",
      "Cost on training data: 0.00850374436571\n",
      "Accuracy on evaluation data: 8487 / 10000\n",
      "\n",
      "Epoch 165 training complete\n",
      "Cost on training data: 0.00844482035067\n",
      "Accuracy on evaluation data: 8486 / 10000\n",
      "\n",
      "Epoch 166 training complete\n",
      "Cost on training data: 0.00838418753763\n",
      "Accuracy on evaluation data: 8490 / 10000\n",
      "\n",
      "Epoch 167 training complete\n",
      "Cost on training data: 0.00832452236419\n",
      "Accuracy on evaluation data: 8489 / 10000\n",
      "\n",
      "Epoch 168 training complete\n",
      "Cost on training data: 0.00826801368804\n",
      "Accuracy on evaluation data: 8488 / 10000\n",
      "\n",
      "Epoch 169 training complete\n",
      "Cost on training data: 0.00820803116989\n",
      "Accuracy on evaluation data: 8487 / 10000\n",
      "\n",
      "Epoch 170 training complete\n",
      "Cost on training data: 0.00815269344163\n",
      "Accuracy on evaluation data: 8488 / 10000\n",
      "\n",
      "Epoch 171 training complete\n",
      "Cost on training data: 0.00809717666068\n",
      "Accuracy on evaluation data: 8492 / 10000\n",
      "\n",
      "Epoch 172 training complete\n",
      "Cost on training data: 0.00804166934591\n",
      "Accuracy on evaluation data: 8491 / 10000\n",
      "\n",
      "Epoch 173 training complete\n",
      "Cost on training data: 0.00798697993715\n",
      "Accuracy on evaluation data: 8492 / 10000\n",
      "\n",
      "Epoch 174 training complete\n",
      "Cost on training data: 0.00793322044453\n",
      "Accuracy on evaluation data: 8489 / 10000\n",
      "\n",
      "Epoch 175 training complete\n",
      "Cost on training data: 0.00788125137879\n",
      "Accuracy on evaluation data: 8493 / 10000\n",
      "\n",
      "Epoch 176 training complete\n",
      "Cost on training data: 0.00782728536155\n",
      "Accuracy on evaluation data: 8491 / 10000\n",
      "\n",
      "Epoch 177 training complete\n",
      "Cost on training data: 0.00777721493638\n",
      "Accuracy on evaluation data: 8489 / 10000\n",
      "\n",
      "Epoch 178 training complete\n",
      "Cost on training data: 0.00772632398799\n",
      "Accuracy on evaluation data: 8491 / 10000\n",
      "\n",
      "Epoch 179 training complete\n",
      "Cost on training data: 0.00767425455675\n",
      "Accuracy on evaluation data: 8492 / 10000\n",
      "\n",
      "Epoch 180 training complete\n",
      "Cost on training data: 0.00762445577193\n",
      "Accuracy on evaluation data: 8493 / 10000\n",
      "\n",
      "Epoch 181 training complete\n",
      "Cost on training data: 0.00757608119238\n",
      "Accuracy on evaluation data: 8492 / 10000\n",
      "\n",
      "Epoch 182 training complete\n",
      "Cost on training data: 0.00752740835517\n",
      "Accuracy on evaluation data: 8492 / 10000\n",
      "\n",
      "Epoch 183 training complete\n",
      "Cost on training data: 0.00748090002185\n",
      "Accuracy on evaluation data: 8495 / 10000\n",
      "\n",
      "Epoch 184 training complete\n",
      "Cost on training data: 0.00743169562502\n",
      "Accuracy on evaluation data: 8491 / 10000\n",
      "\n",
      "Epoch 185 training complete\n",
      "Cost on training data: 0.00738524454842\n",
      "Accuracy on evaluation data: 8490 / 10000\n",
      "\n",
      "Epoch 186 training complete\n",
      "Cost on training data: 0.00733925823936\n",
      "Accuracy on evaluation data: 8492 / 10000\n",
      "\n",
      "Epoch 187 training complete\n",
      "Cost on training data: 0.00729396177111\n",
      "Accuracy on evaluation data: 8494 / 10000\n",
      "\n",
      "Epoch 188 training complete\n",
      "Cost on training data: 0.00724791611773\n",
      "Accuracy on evaluation data: 8493 / 10000\n",
      "\n",
      "Epoch 189 training complete\n",
      "Cost on training data: 0.00720380505683\n",
      "Accuracy on evaluation data: 8494 / 10000\n",
      "\n",
      "Epoch 190 training complete\n",
      "Cost on training data: 0.00715956301965\n",
      "Accuracy on evaluation data: 8494 / 10000\n",
      "\n",
      "Epoch 191 training complete\n",
      "Cost on training data: 0.00711638384484\n",
      "Accuracy on evaluation data: 8492 / 10000\n",
      "\n",
      "Epoch 192 training complete\n",
      "Cost on training data: 0.00707345406641\n",
      "Accuracy on evaluation data: 8492 / 10000\n",
      "\n",
      "Epoch 193 training complete\n",
      "Cost on training data: 0.00703059485893\n",
      "Accuracy on evaluation data: 8490 / 10000\n",
      "\n",
      "Epoch 194 training complete\n",
      "Cost on training data: 0.00698840593202\n",
      "Accuracy on evaluation data: 8490 / 10000\n",
      "\n",
      "Epoch 195 training complete\n",
      "Cost on training data: 0.00694794575941\n",
      "Accuracy on evaluation data: 8486 / 10000\n",
      "\n",
      "Epoch 196 training complete\n",
      "Cost on training data: 0.00690575571287\n",
      "Accuracy on evaluation data: 8488 / 10000\n",
      "\n",
      "Epoch 197 training complete\n",
      "Cost on training data: 0.00686595025501\n",
      "Accuracy on evaluation data: 8494 / 10000\n",
      "\n",
      "Epoch 198 training complete\n",
      "Cost on training data: 0.00682524584463\n",
      "Accuracy on evaluation data: 8496 / 10000\n",
      "\n",
      "Epoch 199 training complete\n",
      "Cost on training data: 0.00678610391666\n",
      "Accuracy on evaluation data: 8497 / 10000\n",
      "\n",
      "Epoch 200 training complete\n",
      "Cost on training data: 0.00674686419314\n",
      "Accuracy on evaluation data: 8495 / 10000\n",
      "\n",
      "Epoch 201 training complete\n",
      "Cost on training data: 0.00670749565692\n",
      "Accuracy on evaluation data: 8495 / 10000\n",
      "\n",
      "Epoch 202 training complete\n",
      "Cost on training data: 0.00666887452108\n",
      "Accuracy on evaluation data: 8496 / 10000\n",
      "\n",
      "Epoch 203 training complete\n",
      "Cost on training data: 0.0066314654017\n",
      "Accuracy on evaluation data: 8492 / 10000\n",
      "\n",
      "Epoch 204 training complete\n",
      "Cost on training data: 0.00659300730627\n",
      "Accuracy on evaluation data: 8498 / 10000\n",
      "\n",
      "Epoch 205 training complete\n",
      "Cost on training data: 0.00655673396443\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 206 training complete\n",
      "Cost on training data: 0.00652107830165\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 207 training complete\n",
      "Cost on training data: 0.00648286007635\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 208 training complete\n",
      "Cost on training data: 0.00644725489581\n",
      "Accuracy on evaluation data: 8500 / 10000\n",
      "\n",
      "Epoch 209 training complete\n",
      "Cost on training data: 0.00641175538304\n",
      "Accuracy on evaluation data: 8497 / 10000\n",
      "\n",
      "Epoch 210 training complete\n",
      "Cost on training data: 0.0063771734532\n",
      "Accuracy on evaluation data: 8502 / 10000\n",
      "\n",
      "Epoch 211 training complete\n",
      "Cost on training data: 0.0063420628639\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 212 training complete\n",
      "Cost on training data: 0.00630736939443\n",
      "Accuracy on evaluation data: 8501 / 10000\n",
      "\n",
      "Epoch 213 training complete\n",
      "Cost on training data: 0.00627380492364\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 214 training complete\n",
      "Cost on training data: 0.00623911911662\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 215 training complete\n",
      "Cost on training data: 0.00620615545159\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 216 training complete\n",
      "Cost on training data: 0.00617296478462\n",
      "Accuracy on evaluation data: 8500 / 10000\n",
      "\n",
      "Epoch 217 training complete\n",
      "Cost on training data: 0.00614049917228\n",
      "Accuracy on evaluation data: 8498 / 10000\n",
      "\n",
      "Epoch 218 training complete\n",
      "Cost on training data: 0.00610781680957\n",
      "Accuracy on evaluation data: 8497 / 10000\n",
      "\n",
      "Epoch 219 training complete\n",
      "Cost on training data: 0.00607616138203\n",
      "Accuracy on evaluation data: 8501 / 10000\n",
      "\n",
      "Epoch 220 training complete\n",
      "Cost on training data: 0.00604421227904\n",
      "Accuracy on evaluation data: 8500 / 10000\n",
      "\n",
      "Epoch 221 training complete\n",
      "Cost on training data: 0.00601330641659\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 222 training complete\n",
      "Cost on training data: 0.00598258126923\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 223 training complete\n",
      "Cost on training data: 0.00595093265168\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 224 training complete\n",
      "Cost on training data: 0.00592110756451\n",
      "Accuracy on evaluation data: 8496 / 10000\n",
      "\n",
      "Epoch 225 training complete\n",
      "Cost on training data: 0.0058904542955\n",
      "Accuracy on evaluation data: 8498 / 10000\n",
      "\n",
      "Epoch 226 training complete\n",
      "Cost on training data: 0.00586057137499\n",
      "Accuracy on evaluation data: 8498 / 10000\n",
      "\n",
      "Epoch 227 training complete\n",
      "Cost on training data: 0.00583076428994\n",
      "Accuracy on evaluation data: 8500 / 10000\n",
      "\n",
      "Epoch 228 training complete\n",
      "Cost on training data: 0.00580152667172\n",
      "Accuracy on evaluation data: 8496 / 10000\n",
      "\n",
      "Epoch 229 training complete\n",
      "Cost on training data: 0.00577255750029\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 230 training complete\n",
      "Cost on training data: 0.00574388301958\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 231 training complete\n",
      "Cost on training data: 0.0057157024523\n",
      "Accuracy on evaluation data: 8500 / 10000\n",
      "\n",
      "Epoch 232 training complete\n",
      "Cost on training data: 0.00568703690085\n",
      "Accuracy on evaluation data: 8500 / 10000\n",
      "\n",
      "Epoch 233 training complete\n",
      "Cost on training data: 0.00565964569515\n",
      "Accuracy on evaluation data: 8496 / 10000\n",
      "\n",
      "Epoch 234 training complete\n",
      "Cost on training data: 0.00563166417825\n",
      "Accuracy on evaluation data: 8497 / 10000\n",
      "\n",
      "Epoch 235 training complete\n",
      "Cost on training data: 0.00560438870146\n",
      "Accuracy on evaluation data: 8501 / 10000\n",
      "\n",
      "Epoch 236 training complete\n",
      "Cost on training data: 0.00557717046191\n",
      "Accuracy on evaluation data: 8498 / 10000\n",
      "\n",
      "Epoch 237 training complete\n",
      "Cost on training data: 0.00555059077674\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 238 training complete\n",
      "Cost on training data: 0.00552382360742\n",
      "Accuracy on evaluation data: 8498 / 10000\n",
      "\n",
      "Epoch 239 training complete\n",
      "Cost on training data: 0.00549777992442\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 240 training complete\n",
      "Cost on training data: 0.00547146953798\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 241 training complete\n",
      "Cost on training data: 0.00544566836651\n",
      "Accuracy on evaluation data: 8497 / 10000\n",
      "\n",
      "Epoch 242 training complete\n",
      "Cost on training data: 0.00542038739637\n",
      "Accuracy on evaluation data: 8498 / 10000\n",
      "\n",
      "Epoch 243 training complete\n",
      "Cost on training data: 0.00539489058192\n",
      "Accuracy on evaluation data: 8501 / 10000\n",
      "\n",
      "Epoch 244 training complete\n",
      "Cost on training data: 0.00536997586985\n",
      "Accuracy on evaluation data: 8500 / 10000\n",
      "\n",
      "Epoch 245 training complete\n",
      "Cost on training data: 0.0053447603044\n",
      "Accuracy on evaluation data: 8500 / 10000\n",
      "\n",
      "Epoch 246 training complete\n",
      "Cost on training data: 0.0053201400607\n",
      "Accuracy on evaluation data: 8500 / 10000\n",
      "\n",
      "Epoch 247 training complete\n",
      "Cost on training data: 0.00529590309335\n",
      "Accuracy on evaluation data: 8501 / 10000\n",
      "\n",
      "Epoch 248 training complete\n",
      "Cost on training data: 0.0052715645772\n",
      "Accuracy on evaluation data: 8504 / 10000\n",
      "\n",
      "Epoch 249 training complete\n",
      "Cost on training data: 0.00524756458661\n",
      "Accuracy on evaluation data: 8503 / 10000\n",
      "\n",
      "Epoch 250 training complete\n",
      "Cost on training data: 0.00522383047791\n",
      "Accuracy on evaluation data: 8502 / 10000\n",
      "\n",
      "Epoch 251 training complete\n",
      "Cost on training data: 0.00520003594573\n",
      "Accuracy on evaluation data: 8502 / 10000\n",
      "\n",
      "Epoch 252 training complete\n",
      "Cost on training data: 0.00517671493548\n",
      "Accuracy on evaluation data: 8503 / 10000\n",
      "\n",
      "Epoch 253 training complete\n",
      "Cost on training data: 0.00515354269626\n",
      "Accuracy on evaluation data: 8501 / 10000\n",
      "\n",
      "Epoch 254 training complete\n",
      "Cost on training data: 0.0051307431091\n",
      "Accuracy on evaluation data: 8503 / 10000\n",
      "\n",
      "Epoch 255 training complete\n",
      "Cost on training data: 0.00510779982893\n",
      "Accuracy on evaluation data: 8502 / 10000\n",
      "\n",
      "Epoch 256 training complete\n",
      "Cost on training data: 0.00508540292435\n",
      "Accuracy on evaluation data: 8502 / 10000\n",
      "\n",
      "Epoch 257 training complete\n",
      "Cost on training data: 0.00506304506157\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 258 training complete\n",
      "Cost on training data: 0.00504084922142\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 259 training complete\n",
      "Cost on training data: 0.00501919173447\n",
      "Accuracy on evaluation data: 8503 / 10000\n",
      "\n",
      "Epoch 260 training complete\n",
      "Cost on training data: 0.00499757075491\n",
      "Accuracy on evaluation data: 8501 / 10000\n",
      "\n",
      "Epoch 261 training complete\n",
      "Cost on training data: 0.00497580631673\n",
      "Accuracy on evaluation data: 8502 / 10000\n",
      "\n",
      "Epoch 262 training complete\n",
      "Cost on training data: 0.0049543291783\n",
      "Accuracy on evaluation data: 8500 / 10000\n",
      "\n",
      "Epoch 263 training complete\n",
      "Cost on training data: 0.00493315428587\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 264 training complete\n",
      "Cost on training data: 0.00491246758924\n",
      "Accuracy on evaluation data: 8498 / 10000\n",
      "\n",
      "Epoch 265 training complete\n",
      "Cost on training data: 0.00489159187432\n",
      "Accuracy on evaluation data: 8501 / 10000\n",
      "\n",
      "Epoch 266 training complete\n",
      "Cost on training data: 0.00487073913102\n",
      "Accuracy on evaluation data: 8503 / 10000\n",
      "\n",
      "Epoch 267 training complete\n",
      "Cost on training data: 0.00485006033382\n",
      "Accuracy on evaluation data: 8503 / 10000\n",
      "\n",
      "Epoch 268 training complete\n",
      "Cost on training data: 0.00482989040882\n",
      "Accuracy on evaluation data: 8501 / 10000\n",
      "\n",
      "Epoch 269 training complete\n",
      "Cost on training data: 0.00480962576189\n",
      "Accuracy on evaluation data: 8504 / 10000\n",
      "\n",
      "Epoch 270 training complete\n",
      "Cost on training data: 0.00478978227128\n",
      "Accuracy on evaluation data: 8505 / 10000\n",
      "\n",
      "Epoch 271 training complete\n",
      "Cost on training data: 0.00477007293426\n",
      "Accuracy on evaluation data: 8507 / 10000\n",
      "\n",
      "Epoch 272 training complete\n",
      "Cost on training data: 0.00475044661541\n",
      "Accuracy on evaluation data: 8509 / 10000\n",
      "\n",
      "Epoch 273 training complete\n",
      "Cost on training data: 0.00473065144784\n",
      "Accuracy on evaluation data: 8506 / 10000\n",
      "\n",
      "Epoch 274 training complete\n",
      "Cost on training data: 0.00471142601194\n",
      "Accuracy on evaluation data: 8505 / 10000\n",
      "\n",
      "Epoch 275 training complete\n",
      "Cost on training data: 0.00469227656663\n",
      "Accuracy on evaluation data: 8509 / 10000\n",
      "\n",
      "Epoch 276 training complete\n",
      "Cost on training data: 0.00467315872565\n",
      "Accuracy on evaluation data: 8507 / 10000\n",
      "\n",
      "Epoch 277 training complete\n",
      "Cost on training data: 0.00465443130259\n",
      "Accuracy on evaluation data: 8509 / 10000\n",
      "\n",
      "Epoch 278 training complete\n",
      "Cost on training data: 0.00463578427215\n",
      "Accuracy on evaluation data: 8509 / 10000\n",
      "\n",
      "Epoch 279 training complete\n",
      "Cost on training data: 0.00461713383386\n",
      "Accuracy on evaluation data: 8507 / 10000\n",
      "\n",
      "Epoch 280 training complete\n",
      "Cost on training data: 0.00459874399901\n",
      "Accuracy on evaluation data: 8508 / 10000\n",
      "\n",
      "Epoch 281 training complete\n",
      "Cost on training data: 0.00458070751748\n",
      "Accuracy on evaluation data: 8509 / 10000\n",
      "\n",
      "Epoch 282 training complete\n",
      "Cost on training data: 0.00456242056351\n",
      "Accuracy on evaluation data: 8511 / 10000\n",
      "\n",
      "Epoch 283 training complete\n",
      "Cost on training data: 0.00454427919238\n",
      "Accuracy on evaluation data: 8512 / 10000\n",
      "\n",
      "Epoch 284 training complete\n",
      "Cost on training data: 0.00452647580006\n",
      "Accuracy on evaluation data: 8511 / 10000\n",
      "\n",
      "Epoch 285 training complete\n",
      "Cost on training data: 0.00450882332983\n",
      "Accuracy on evaluation data: 8512 / 10000\n",
      "\n",
      "Epoch 286 training complete\n",
      "Cost on training data: 0.00449128053681\n",
      "Accuracy on evaluation data: 8512 / 10000\n",
      "\n",
      "Epoch 287 training complete\n",
      "Cost on training data: 0.00447389215648\n",
      "Accuracy on evaluation data: 8512 / 10000\n",
      "\n",
      "Epoch 288 training complete\n",
      "Cost on training data: 0.00445670242945\n",
      "Accuracy on evaluation data: 8513 / 10000\n",
      "\n",
      "Epoch 289 training complete\n",
      "Cost on training data: 0.004439506414\n",
      "Accuracy on evaluation data: 8515 / 10000\n",
      "\n",
      "Epoch 290 training complete\n",
      "Cost on training data: 0.00442258713164\n",
      "Accuracy on evaluation data: 8514 / 10000\n",
      "\n",
      "Epoch 291 training complete\n",
      "Cost on training data: 0.00440552753573\n",
      "Accuracy on evaluation data: 8515 / 10000\n",
      "\n",
      "Epoch 292 training complete\n",
      "Cost on training data: 0.00438889965471\n",
      "Accuracy on evaluation data: 8516 / 10000\n",
      "\n",
      "Epoch 293 training complete\n",
      "Cost on training data: 0.0043723626098\n",
      "Accuracy on evaluation data: 8515 / 10000\n",
      "\n",
      "Epoch 294 training complete\n",
      "Cost on training data: 0.00435557393591\n",
      "Accuracy on evaluation data: 8517 / 10000\n",
      "\n",
      "Epoch 295 training complete\n",
      "Cost on training data: 0.00433930591997\n",
      "Accuracy on evaluation data: 8512 / 10000\n",
      "\n",
      "Epoch 296 training complete\n",
      "Cost on training data: 0.00432307653524\n",
      "Accuracy on evaluation data: 8518 / 10000\n",
      "\n",
      "Epoch 297 training complete\n",
      "Cost on training data: 0.00430695326644\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 298 training complete\n",
      "Cost on training data: 0.00429079110517\n",
      "Accuracy on evaluation data: 8518 / 10000\n",
      "\n",
      "Epoch 299 training complete\n",
      "Cost on training data: 0.00427491593416\n",
      "Accuracy on evaluation data: 8518 / 10000\n",
      "\n",
      "Epoch 300 training complete\n",
      "Cost on training data: 0.00425918402703\n",
      "Accuracy on evaluation data: 8518 / 10000\n",
      "\n",
      "Epoch 301 training complete\n",
      "Cost on training data: 0.00424343132512\n",
      "Accuracy on evaluation data: 8516 / 10000\n",
      "\n",
      "Epoch 302 training complete\n",
      "Cost on training data: 0.00422776496014\n",
      "Accuracy on evaluation data: 8514 / 10000\n",
      "\n",
      "Epoch 303 training complete\n",
      "Cost on training data: 0.00421238286274\n",
      "Accuracy on evaluation data: 8514 / 10000\n",
      "\n",
      "Epoch 304 training complete\n",
      "Cost on training data: 0.00419703292428\n",
      "Accuracy on evaluation data: 8516 / 10000\n",
      "\n",
      "Epoch 305 training complete\n",
      "Cost on training data: 0.00418171518658\n",
      "Accuracy on evaluation data: 8515 / 10000\n",
      "\n",
      "Epoch 306 training complete\n",
      "Cost on training data: 0.00416661910775\n",
      "Accuracy on evaluation data: 8516 / 10000\n",
      "\n",
      "Epoch 307 training complete\n",
      "Cost on training data: 0.00415175625389\n",
      "Accuracy on evaluation data: 8515 / 10000\n",
      "\n",
      "Epoch 308 training complete\n",
      "Cost on training data: 0.00413672731345\n",
      "Accuracy on evaluation data: 8516 / 10000\n",
      "\n",
      "Epoch 309 training complete\n",
      "Cost on training data: 0.00412194562522\n",
      "Accuracy on evaluation data: 8515 / 10000\n",
      "\n",
      "Epoch 310 training complete\n",
      "Cost on training data: 0.00410718645492\n",
      "Accuracy on evaluation data: 8515 / 10000\n",
      "\n",
      "Epoch 311 training complete\n",
      "Cost on training data: 0.00409259523113\n",
      "Accuracy on evaluation data: 8516 / 10000\n",
      "\n",
      "Epoch 312 training complete\n",
      "Cost on training data: 0.00407809575952\n",
      "Accuracy on evaluation data: 8516 / 10000\n",
      "\n",
      "Epoch 313 training complete\n",
      "Cost on training data: 0.00406375386541\n",
      "Accuracy on evaluation data: 8517 / 10000\n",
      "\n",
      "Epoch 314 training complete\n",
      "Cost on training data: 0.004049405138\n",
      "Accuracy on evaluation data: 8516 / 10000\n",
      "\n",
      "Epoch 315 training complete\n",
      "Cost on training data: 0.00403530532464\n",
      "Accuracy on evaluation data: 8517 / 10000\n",
      "\n",
      "Epoch 316 training complete\n",
      "Cost on training data: 0.00402104160484\n",
      "Accuracy on evaluation data: 8517 / 10000\n",
      "\n",
      "Epoch 317 training complete\n",
      "Cost on training data: 0.00400702950505\n",
      "Accuracy on evaluation data: 8518 / 10000\n",
      "\n",
      "Epoch 318 training complete\n",
      "Cost on training data: 0.00399312959081\n",
      "Accuracy on evaluation data: 8516 / 10000\n",
      "\n",
      "Epoch 319 training complete\n",
      "Cost on training data: 0.00397930536022\n",
      "Accuracy on evaluation data: 8516 / 10000\n",
      "\n",
      "Epoch 320 training complete\n",
      "Cost on training data: 0.00396544344321\n",
      "Accuracy on evaluation data: 8517 / 10000\n",
      "\n",
      "Epoch 321 training complete\n",
      "Cost on training data: 0.00395188838739\n",
      "Accuracy on evaluation data: 8517 / 10000\n",
      "\n",
      "Epoch 322 training complete\n",
      "Cost on training data: 0.00393829782999\n",
      "Accuracy on evaluation data: 8517 / 10000\n",
      "\n",
      "Epoch 323 training complete\n",
      "Cost on training data: 0.00392487588234\n",
      "Accuracy on evaluation data: 8518 / 10000\n",
      "\n",
      "Epoch 324 training complete\n",
      "Cost on training data: 0.00391152760357\n",
      "Accuracy on evaluation data: 8516 / 10000\n",
      "\n",
      "Epoch 325 training complete\n",
      "Cost on training data: 0.00389817129047\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 326 training complete\n",
      "Cost on training data: 0.00388502293807\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 327 training complete\n",
      "Cost on training data: 0.00387174060136\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 328 training complete\n",
      "Cost on training data: 0.00385883522904\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 329 training complete\n",
      "Cost on training data: 0.00384581207099\n",
      "Accuracy on evaluation data: 8518 / 10000\n",
      "\n",
      "Epoch 330 training complete\n",
      "Cost on training data: 0.0038329694414\n",
      "Accuracy on evaluation data: 8517 / 10000\n",
      "\n",
      "Epoch 331 training complete\n",
      "Cost on training data: 0.00382039080798\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 332 training complete\n",
      "Cost on training data: 0.00380742794893\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 333 training complete\n",
      "Cost on training data: 0.00379481749483\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 334 training complete\n",
      "Cost on training data: 0.00378223492295\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 335 training complete\n",
      "Cost on training data: 0.00376979677338\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 336 training complete\n",
      "Cost on training data: 0.00375746531273\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 337 training complete\n",
      "Cost on training data: 0.00374502317777\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 338 training complete\n",
      "Cost on training data: 0.00373279768397\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 339 training complete\n",
      "Cost on training data: 0.00372067018421\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 340 training complete\n",
      "Cost on training data: 0.00370853841246\n",
      "Accuracy on evaluation data: 8518 / 10000\n",
      "\n",
      "Epoch 341 training complete\n",
      "Cost on training data: 0.00369657486527\n",
      "Accuracy on evaluation data: 8518 / 10000\n",
      "\n",
      "Epoch 342 training complete\n",
      "Cost on training data: 0.0036845685233\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 343 training complete\n",
      "Cost on training data: 0.00367275849665\n",
      "Accuracy on evaluation data: 8518 / 10000\n",
      "\n",
      "Epoch 344 training complete\n",
      "Cost on training data: 0.00366085079238\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 345 training complete\n",
      "Cost on training data: 0.00364914364571\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 346 training complete\n",
      "Cost on training data: 0.00363754891026\n",
      "Accuracy on evaluation data: 8518 / 10000\n",
      "\n",
      "Epoch 347 training complete\n",
      "Cost on training data: 0.00362591732734\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 348 training complete\n",
      "Cost on training data: 0.00361444349415\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 349 training complete\n",
      "Cost on training data: 0.00360298181596\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 350 training complete\n",
      "Cost on training data: 0.00359152807718\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 351 training complete\n",
      "Cost on training data: 0.00358022438355\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 352 training complete\n",
      "Cost on training data: 0.0035689705447\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 353 training complete\n",
      "Cost on training data: 0.0035578181596\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 354 training complete\n",
      "Cost on training data: 0.00354666346631\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 355 training complete\n",
      "Cost on training data: 0.003535607783\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 356 training complete\n",
      "Cost on training data: 0.00352463297097\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "\n",
      "Epoch 357 training complete\n",
      "Cost on training data: 0.0035137029455\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 358 training complete\n",
      "Cost on training data: 0.00350285505946\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 359 training complete\n",
      "Cost on training data: 0.00349207965896\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "\n",
      "Epoch 360 training complete\n",
      "Cost on training data: 0.00348136005084\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 361 training complete\n",
      "Cost on training data: 0.00347062854887\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 362 training complete\n",
      "Cost on training data: 0.00345999512727\n",
      "Accuracy on evaluation data: 8517 / 10000\n",
      "\n",
      "Epoch 363 training complete\n",
      "Cost on training data: 0.0034495394331\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 364 training complete\n",
      "Cost on training data: 0.00343894307513\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 365 training complete\n",
      "Cost on training data: 0.00342856687278\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 366 training complete\n",
      "Cost on training data: 0.00341818217044\n",
      "Accuracy on evaluation data: 8523 / 10000\n",
      "\n",
      "Epoch 367 training complete\n",
      "Cost on training data: 0.00340791818999\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 368 training complete\n",
      "Cost on training data: 0.00339764259602\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 369 training complete\n",
      "Cost on training data: 0.00338741120091\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 370 training complete\n",
      "Cost on training data: 0.00337725755863\n",
      "Accuracy on evaluation data: 8523 / 10000\n",
      "\n",
      "Epoch 371 training complete\n",
      "Cost on training data: 0.00336714486171\n",
      "Accuracy on evaluation data: 8523 / 10000\n",
      "\n",
      "Epoch 372 training complete\n",
      "Cost on training data: 0.00335710847054\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 373 training complete\n",
      "Cost on training data: 0.00334712885168\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 374 training complete\n",
      "Cost on training data: 0.00333726697541\n",
      "Accuracy on evaluation data: 8523 / 10000\n",
      "\n",
      "Epoch 375 training complete\n",
      "Cost on training data: 0.00332739340704\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "\n",
      "Epoch 376 training complete\n",
      "Cost on training data: 0.00331755803365\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "\n",
      "Epoch 377 training complete\n",
      "Cost on training data: 0.00330775843245\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 378 training complete\n",
      "Cost on training data: 0.00329809327076\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 379 training complete\n",
      "Cost on training data: 0.00328843449435\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 380 training complete\n",
      "Cost on training data: 0.00327881809234\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 381 training complete\n",
      "Cost on training data: 0.00326931006797\n",
      "Accuracy on evaluation data: 8523 / 10000\n",
      "\n",
      "Epoch 382 training complete\n",
      "Cost on training data: 0.00325980914949\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "\n",
      "Epoch 383 training complete\n",
      "Cost on training data: 0.00325035387299\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "\n",
      "Epoch 384 training complete\n",
      "Cost on training data: 0.00324099654148\n",
      "Accuracy on evaluation data: 8523 / 10000\n",
      "\n",
      "Epoch 385 training complete\n",
      "Cost on training data: 0.00323161433737\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "\n",
      "Epoch 386 training complete\n",
      "Cost on training data: 0.00322234253857\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "\n",
      "Epoch 387 training complete\n",
      "Cost on training data: 0.00321309388771\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "\n",
      "Epoch 388 training complete\n",
      "Cost on training data: 0.00320391385151\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 389 training complete\n",
      "Cost on training data: 0.00319480571416\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 390 training complete\n",
      "Cost on training data: 0.00318571869327\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "\n",
      "Epoch 391 training complete\n",
      "Cost on training data: 0.00317666862502\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "\n",
      "Epoch 392 training complete\n",
      "Cost on training data: 0.00316766065163\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 393 training complete\n",
      "Cost on training data: 0.00315872892661\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "\n",
      "Epoch 394 training complete\n",
      "Cost on training data: 0.0031498650823\n",
      "Accuracy on evaluation data: 8521 / 10000\n",
      "\n",
      "Epoch 395 training complete\n",
      "Cost on training data: 0.00314104754583\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n",
      "Epoch 396 training complete\n",
      "Cost on training data: 0.00313219207874\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 397 training complete\n",
      "Cost on training data: 0.00312342390333\n",
      "Accuracy on evaluation data: 8519 / 10000\n",
      "\n",
      "Epoch 398 training complete\n",
      "Cost on training data: 0.00311473347416\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 399 training complete\n",
      "Cost on training data: 0.00310610307266\n",
      "Accuracy on evaluation data: 8522 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [6202,\n",
       "  6893,\n",
       "  7166,\n",
       "  7723,\n",
       "  7778,\n",
       "  7889,\n",
       "  8023,\n",
       "  8049,\n",
       "  8181,\n",
       "  8105,\n",
       "  8208,\n",
       "  8248,\n",
       "  8266,\n",
       "  8292,\n",
       "  8291,\n",
       "  8330,\n",
       "  8320,\n",
       "  8348,\n",
       "  8364,\n",
       "  8355,\n",
       "  8340,\n",
       "  8317,\n",
       "  8384,\n",
       "  8388,\n",
       "  8368,\n",
       "  8378,\n",
       "  8398,\n",
       "  8378,\n",
       "  8401,\n",
       "  8416,\n",
       "  8428,\n",
       "  8403,\n",
       "  8434,\n",
       "  8421,\n",
       "  8411,\n",
       "  8429,\n",
       "  8440,\n",
       "  8436,\n",
       "  8432,\n",
       "  8434,\n",
       "  8429,\n",
       "  8426,\n",
       "  8430,\n",
       "  8439,\n",
       "  8439,\n",
       "  8439,\n",
       "  8460,\n",
       "  8447,\n",
       "  8444,\n",
       "  8438,\n",
       "  8450,\n",
       "  8436,\n",
       "  8431,\n",
       "  8444,\n",
       "  8441,\n",
       "  8443,\n",
       "  8448,\n",
       "  8449,\n",
       "  8448,\n",
       "  8449,\n",
       "  8448,\n",
       "  8447,\n",
       "  8445,\n",
       "  8443,\n",
       "  8448,\n",
       "  8444,\n",
       "  8446,\n",
       "  8446,\n",
       "  8432,\n",
       "  8452,\n",
       "  8445,\n",
       "  8444,\n",
       "  8442,\n",
       "  8447,\n",
       "  8444,\n",
       "  8454,\n",
       "  8448,\n",
       "  8456,\n",
       "  8453,\n",
       "  8449,\n",
       "  8444,\n",
       "  8449,\n",
       "  8446,\n",
       "  8444,\n",
       "  8461,\n",
       "  8453,\n",
       "  8456,\n",
       "  8447,\n",
       "  8452,\n",
       "  8455,\n",
       "  8445,\n",
       "  8455,\n",
       "  8460,\n",
       "  8453,\n",
       "  8463,\n",
       "  8460,\n",
       "  8462,\n",
       "  8463,\n",
       "  8461,\n",
       "  8468,\n",
       "  8468,\n",
       "  8467,\n",
       "  8471,\n",
       "  8471,\n",
       "  8469,\n",
       "  8470,\n",
       "  8472,\n",
       "  8472,\n",
       "  8477,\n",
       "  8470,\n",
       "  8470,\n",
       "  8475,\n",
       "  8473,\n",
       "  8476,\n",
       "  8474,\n",
       "  8476,\n",
       "  8481,\n",
       "  8481,\n",
       "  8478,\n",
       "  8477,\n",
       "  8477,\n",
       "  8480,\n",
       "  8482,\n",
       "  8479,\n",
       "  8478,\n",
       "  8482,\n",
       "  8479,\n",
       "  8480,\n",
       "  8474,\n",
       "  8479,\n",
       "  8481,\n",
       "  8479,\n",
       "  8481,\n",
       "  8481,\n",
       "  8483,\n",
       "  8479,\n",
       "  8482,\n",
       "  8483,\n",
       "  8480,\n",
       "  8481,\n",
       "  8480,\n",
       "  8486,\n",
       "  8484,\n",
       "  8483,\n",
       "  8482,\n",
       "  8480,\n",
       "  8481,\n",
       "  8482,\n",
       "  8482,\n",
       "  8482,\n",
       "  8483,\n",
       "  8487,\n",
       "  8485,\n",
       "  8481,\n",
       "  8484,\n",
       "  8484,\n",
       "  8486,\n",
       "  8488,\n",
       "  8486,\n",
       "  8487,\n",
       "  8487,\n",
       "  8487,\n",
       "  8487,\n",
       "  8489,\n",
       "  8487,\n",
       "  8486,\n",
       "  8490,\n",
       "  8489,\n",
       "  8488,\n",
       "  8487,\n",
       "  8488,\n",
       "  8492,\n",
       "  8491,\n",
       "  8492,\n",
       "  8489,\n",
       "  8493,\n",
       "  8491,\n",
       "  8489,\n",
       "  8491,\n",
       "  8492,\n",
       "  8493,\n",
       "  8492,\n",
       "  8492,\n",
       "  8495,\n",
       "  8491,\n",
       "  8490,\n",
       "  8492,\n",
       "  8494,\n",
       "  8493,\n",
       "  8494,\n",
       "  8494,\n",
       "  8492,\n",
       "  8492,\n",
       "  8490,\n",
       "  8490,\n",
       "  8486,\n",
       "  8488,\n",
       "  8494,\n",
       "  8496,\n",
       "  8497,\n",
       "  8495,\n",
       "  8495,\n",
       "  8496,\n",
       "  8492,\n",
       "  8498,\n",
       "  8499,\n",
       "  8499,\n",
       "  8499,\n",
       "  8500,\n",
       "  8497,\n",
       "  8502,\n",
       "  8499,\n",
       "  8501,\n",
       "  8499,\n",
       "  8499,\n",
       "  8499,\n",
       "  8500,\n",
       "  8498,\n",
       "  8497,\n",
       "  8501,\n",
       "  8500,\n",
       "  8499,\n",
       "  8499,\n",
       "  8499,\n",
       "  8496,\n",
       "  8498,\n",
       "  8498,\n",
       "  8500,\n",
       "  8496,\n",
       "  8499,\n",
       "  8499,\n",
       "  8500,\n",
       "  8500,\n",
       "  8496,\n",
       "  8497,\n",
       "  8501,\n",
       "  8498,\n",
       "  8499,\n",
       "  8498,\n",
       "  8499,\n",
       "  8499,\n",
       "  8497,\n",
       "  8498,\n",
       "  8501,\n",
       "  8500,\n",
       "  8500,\n",
       "  8500,\n",
       "  8501,\n",
       "  8504,\n",
       "  8503,\n",
       "  8502,\n",
       "  8502,\n",
       "  8503,\n",
       "  8501,\n",
       "  8503,\n",
       "  8502,\n",
       "  8502,\n",
       "  8499,\n",
       "  8499,\n",
       "  8503,\n",
       "  8501,\n",
       "  8502,\n",
       "  8500,\n",
       "  8499,\n",
       "  8498,\n",
       "  8501,\n",
       "  8503,\n",
       "  8503,\n",
       "  8501,\n",
       "  8504,\n",
       "  8505,\n",
       "  8507,\n",
       "  8509,\n",
       "  8506,\n",
       "  8505,\n",
       "  8509,\n",
       "  8507,\n",
       "  8509,\n",
       "  8509,\n",
       "  8507,\n",
       "  8508,\n",
       "  8509,\n",
       "  8511,\n",
       "  8512,\n",
       "  8511,\n",
       "  8512,\n",
       "  8512,\n",
       "  8512,\n",
       "  8513,\n",
       "  8515,\n",
       "  8514,\n",
       "  8515,\n",
       "  8516,\n",
       "  8515,\n",
       "  8517,\n",
       "  8512,\n",
       "  8518,\n",
       "  8519,\n",
       "  8518,\n",
       "  8518,\n",
       "  8518,\n",
       "  8516,\n",
       "  8514,\n",
       "  8514,\n",
       "  8516,\n",
       "  8515,\n",
       "  8516,\n",
       "  8515,\n",
       "  8516,\n",
       "  8515,\n",
       "  8515,\n",
       "  8516,\n",
       "  8516,\n",
       "  8517,\n",
       "  8516,\n",
       "  8517,\n",
       "  8517,\n",
       "  8518,\n",
       "  8516,\n",
       "  8516,\n",
       "  8517,\n",
       "  8517,\n",
       "  8517,\n",
       "  8518,\n",
       "  8516,\n",
       "  8519,\n",
       "  8519,\n",
       "  8519,\n",
       "  8519,\n",
       "  8518,\n",
       "  8517,\n",
       "  8520,\n",
       "  8519,\n",
       "  8520,\n",
       "  8522,\n",
       "  8522,\n",
       "  8520,\n",
       "  8519,\n",
       "  8519,\n",
       "  8519,\n",
       "  8518,\n",
       "  8518,\n",
       "  8520,\n",
       "  8518,\n",
       "  8520,\n",
       "  8519,\n",
       "  8518,\n",
       "  8519,\n",
       "  8519,\n",
       "  8519,\n",
       "  8520,\n",
       "  8519,\n",
       "  8520,\n",
       "  8519,\n",
       "  8520,\n",
       "  8520,\n",
       "  8521,\n",
       "  8519,\n",
       "  8520,\n",
       "  8521,\n",
       "  8522,\n",
       "  8522,\n",
       "  8517,\n",
       "  8522,\n",
       "  8522,\n",
       "  8520,\n",
       "  8523,\n",
       "  8519,\n",
       "  8522,\n",
       "  8522,\n",
       "  8523,\n",
       "  8523,\n",
       "  8522,\n",
       "  8522,\n",
       "  8523,\n",
       "  8521,\n",
       "  8521,\n",
       "  8522,\n",
       "  8522,\n",
       "  8522,\n",
       "  8522,\n",
       "  8523,\n",
       "  8521,\n",
       "  8521,\n",
       "  8523,\n",
       "  8521,\n",
       "  8521,\n",
       "  8521,\n",
       "  8522,\n",
       "  8522,\n",
       "  8521,\n",
       "  8521,\n",
       "  8520,\n",
       "  8521,\n",
       "  8521,\n",
       "  8522,\n",
       "  8519,\n",
       "  8519,\n",
       "  8520,\n",
       "  8522],\n",
       " [1.8413956554297912,\n",
       "  1.3858041717072307,\n",
       "  1.1546725338441977,\n",
       "  0.91901869393148139,\n",
       "  0.81751180155231939,\n",
       "  0.67193699903001014,\n",
       "  0.5531305711156802,\n",
       "  0.50795476893449254,\n",
       "  0.43731063676652132,\n",
       "  0.41183973611134034,\n",
       "  0.35693606980064235,\n",
       "  0.31178441358405634,\n",
       "  0.28114843983954924,\n",
       "  0.26487647272206383,\n",
       "  0.2337282803252082,\n",
       "  0.21703514339779223,\n",
       "  0.20222480197600073,\n",
       "  0.18062382846064604,\n",
       "  0.16933616601559179,\n",
       "  0.16502900684980323,\n",
       "  0.1575843717690881,\n",
       "  0.15260889593994664,\n",
       "  0.13310816615327092,\n",
       "  0.12420084568879275,\n",
       "  0.11861432193320399,\n",
       "  0.1120025089671066,\n",
       "  0.10613158509085809,\n",
       "  0.10312668342983351,\n",
       "  0.094575390790800282,\n",
       "  0.090938506423323726,\n",
       "  0.087158518154811473,\n",
       "  0.083921626524160048,\n",
       "  0.079560361054684475,\n",
       "  0.076573608350482714,\n",
       "  0.074390233175254422,\n",
       "  0.070220521719122603,\n",
       "  0.067432073658404701,\n",
       "  0.065135148020056294,\n",
       "  0.062963780293170957,\n",
       "  0.060491176200405693,\n",
       "  0.059016723162585424,\n",
       "  0.056355517195609779,\n",
       "  0.054292902011062572,\n",
       "  0.051987566596689495,\n",
       "  0.049988902078805651,\n",
       "  0.048961401773249334,\n",
       "  0.047341772985141084,\n",
       "  0.045552813721030823,\n",
       "  0.044809160219101703,\n",
       "  0.0433551198074342,\n",
       "  0.042091452655680817,\n",
       "  0.041197051999432931,\n",
       "  0.03974116784931761,\n",
       "  0.038821152771333807,\n",
       "  0.038064805879259762,\n",
       "  0.036800689839476555,\n",
       "  0.035805802143667666,\n",
       "  0.035116890975696674,\n",
       "  0.034169414774680072,\n",
       "  0.033315995153034489,\n",
       "  0.032540840198802164,\n",
       "  0.031732864025377172,\n",
       "  0.031088382354721913,\n",
       "  0.030250674556358194,\n",
       "  0.029688589242091373,\n",
       "  0.029063100012820631,\n",
       "  0.028464011771739156,\n",
       "  0.027836026254327378,\n",
       "  0.027451145964823908,\n",
       "  0.026797311922476121,\n",
       "  0.026467579725479598,\n",
       "  0.025839915482789053,\n",
       "  0.025334615415506152,\n",
       "  0.02487916165077424,\n",
       "  0.024393858434531891,\n",
       "  0.02406964654617225,\n",
       "  0.02354051355895705,\n",
       "  0.023182284272263581,\n",
       "  0.022703999338329433,\n",
       "  0.022349517303408523,\n",
       "  0.021904071671335998,\n",
       "  0.021515707468915866,\n",
       "  0.021115121191969795,\n",
       "  0.020729586599146366,\n",
       "  0.020360226355424879,\n",
       "  0.019889231757220144,\n",
       "  0.019480146043905938,\n",
       "  0.01911545505811716,\n",
       "  0.018764889080296415,\n",
       "  0.018481945359208062,\n",
       "  0.018141988263832873,\n",
       "  0.017848616608896826,\n",
       "  0.017590422933569844,\n",
       "  0.017335230725445284,\n",
       "  0.017056027491952567,\n",
       "  0.016843529461177127,\n",
       "  0.016572634256902124,\n",
       "  0.016342410475858254,\n",
       "  0.016148608185967417,\n",
       "  0.015919025049295633,\n",
       "  0.015710926123819784,\n",
       "  0.015504492642004765,\n",
       "  0.015319936316147512,\n",
       "  0.015127153116353289,\n",
       "  0.014949464824062741,\n",
       "  0.014754119885399604,\n",
       "  0.014581516031295717,\n",
       "  0.014418393904544996,\n",
       "  0.014242126832867826,\n",
       "  0.014096634425348469,\n",
       "  0.013917252564353368,\n",
       "  0.013756483480013657,\n",
       "  0.01359860977761131,\n",
       "  0.013450917838139868,\n",
       "  0.01330015349119288,\n",
       "  0.013168870362077749,\n",
       "  0.013013930959705905,\n",
       "  0.012877505108361867,\n",
       "  0.012733843729215951,\n",
       "  0.012616788923606952,\n",
       "  0.012466518985761435,\n",
       "  0.012338499760650004,\n",
       "  0.01221774032303637,\n",
       "  0.012082568994029277,\n",
       "  0.011955788055542448,\n",
       "  0.011837555243981057,\n",
       "  0.01171571286606266,\n",
       "  0.011611698772379811,\n",
       "  0.011481292115494432,\n",
       "  0.011367304653454544,\n",
       "  0.011261072178536596,\n",
       "  0.011149494942498055,\n",
       "  0.011043600596075006,\n",
       "  0.010941975932455036,\n",
       "  0.010840294296180908,\n",
       "  0.0107431858863894,\n",
       "  0.010639555840032826,\n",
       "  0.010545830909144051,\n",
       "  0.010452627830047424,\n",
       "  0.010358991422786682,\n",
       "  0.010268631822192724,\n",
       "  0.010181347321506055,\n",
       "  0.010095071203816695,\n",
       "  0.010009734993908798,\n",
       "  0.0099250428013403381,\n",
       "  0.0098432094234493818,\n",
       "  0.0097641694687428428,\n",
       "  0.0096825848084769461,\n",
       "  0.009603247535611802,\n",
       "  0.0095267850422297989,\n",
       "  0.0094493985915673519,\n",
       "  0.0093774512394151233,\n",
       "  0.0093025652023725255,\n",
       "  0.0092302935775249971,\n",
       "  0.009159678514781602,\n",
       "  0.0090916744908447408,\n",
       "  0.0090220589728012487,\n",
       "  0.0089522277962942945,\n",
       "  0.0088869344794486604,\n",
       "  0.0088192549157635261,\n",
       "  0.0087552123211847681,\n",
       "  0.0086923427567159508,\n",
       "  0.0086271919348382235,\n",
       "  0.0085639661692566531,\n",
       "  0.0085037443657109968,\n",
       "  0.0084448203506662465,\n",
       "  0.0083841875376348806,\n",
       "  0.0083245223641942815,\n",
       "  0.0082680136880415121,\n",
       "  0.0082080311698875322,\n",
       "  0.0081526934416282833,\n",
       "  0.0080971766606788807,\n",
       "  0.008041669345912163,\n",
       "  0.0079869799371481993,\n",
       "  0.0079332204445262695,\n",
       "  0.0078812513787912299,\n",
       "  0.0078272853615520659,\n",
       "  0.0077772149363810007,\n",
       "  0.0077263239879927451,\n",
       "  0.007674254556754731,\n",
       "  0.0076244557719334222,\n",
       "  0.0075760811923823784,\n",
       "  0.0075274083551687869,\n",
       "  0.0074809000218490594,\n",
       "  0.0074316956250197628,\n",
       "  0.0073852445484225747,\n",
       "  0.0073392582393617522,\n",
       "  0.0072939617711092379,\n",
       "  0.0072479161177343796,\n",
       "  0.0072038050568336382,\n",
       "  0.007159563019646039,\n",
       "  0.0071163838448360358,\n",
       "  0.0070734540664130086,\n",
       "  0.0070305948589269108,\n",
       "  0.0069884059320150143,\n",
       "  0.0069479457594088011,\n",
       "  0.0069057557128688442,\n",
       "  0.0068659502550120766,\n",
       "  0.0068252458446261899,\n",
       "  0.0067861039166581729,\n",
       "  0.0067468641931425297,\n",
       "  0.006707495656924441,\n",
       "  0.0066688745210794053,\n",
       "  0.006631465401698137,\n",
       "  0.006593007306267171,\n",
       "  0.0065567339644260298,\n",
       "  0.0065210783016456872,\n",
       "  0.0064828600763474241,\n",
       "  0.0064472548958143033,\n",
       "  0.006411755383036879,\n",
       "  0.0063771734531976798,\n",
       "  0.0063420628639021615,\n",
       "  0.0063073693944288352,\n",
       "  0.0062738049236371477,\n",
       "  0.0062391191166248309,\n",
       "  0.0062061554515933084,\n",
       "  0.0061729647846170341,\n",
       "  0.0061404991722755582,\n",
       "  0.0061078168095678211,\n",
       "  0.0060761613820304134,\n",
       "  0.0060442122790415678,\n",
       "  0.0060133064165948293,\n",
       "  0.0059825812692334713,\n",
       "  0.0059509326516822897,\n",
       "  0.0059211075645093014,\n",
       "  0.0058904542955027298,\n",
       "  0.0058605713749888084,\n",
       "  0.0058307642899384157,\n",
       "  0.005801526671723815,\n",
       "  0.0057725575002922783,\n",
       "  0.0057438830195766727,\n",
       "  0.0057157024522996574,\n",
       "  0.005687036900846168,\n",
       "  0.005659645695146133,\n",
       "  0.0056316641782465657,\n",
       "  0.0056043887014637726,\n",
       "  0.0055771704619130506,\n",
       "  0.0055505907767428369,\n",
       "  0.0055238236074212224,\n",
       "  0.0054977799244214738,\n",
       "  0.0054714695379827734,\n",
       "  0.0054456683665062436,\n",
       "  0.0054203873963743505,\n",
       "  0.0053948905819164927,\n",
       "  0.0053699758698482012,\n",
       "  0.0053447603043986214,\n",
       "  0.0053201400606977881,\n",
       "  0.0052959030933524016,\n",
       "  0.0052715645772023496,\n",
       "  0.0052475645866068921,\n",
       "  0.0052238304779148297,\n",
       "  0.0052000359457253969,\n",
       "  0.0051767149354838725,\n",
       "  0.005153542696258717,\n",
       "  0.0051307431091041408,\n",
       "  0.0051077998289278731,\n",
       "  0.0050854029243479638,\n",
       "  0.0050630450615705216,\n",
       "  0.0050408492214216102,\n",
       "  0.0050191917344682728,\n",
       "  0.0049975707549106616,\n",
       "  0.0049758063167321086,\n",
       "  0.0049543291783033178,\n",
       "  0.0049331542858704154,\n",
       "  0.0049124675892405576,\n",
       "  0.0048915918743179954,\n",
       "  0.0048707391310163529,\n",
       "  0.0048500603338162548,\n",
       "  0.0048298904088213595,\n",
       "  0.0048096257618889388,\n",
       "  0.0047897822712810942,\n",
       "  0.0047700729342629649,\n",
       "  0.004750446615406456,\n",
       "  0.0047306514478421945,\n",
       "  0.0047114260119364313,\n",
       "  0.004692276566625359,\n",
       "  0.0046731587256464398,\n",
       "  0.0046544313025938091,\n",
       "  0.0046357842721531225,\n",
       "  0.0046171338338642457,\n",
       "  0.0045987439990057702,\n",
       "  0.0045807075174798383,\n",
       "  0.0045624205635074319,\n",
       "  0.0045442791923792384,\n",
       "  0.004526475800061971,\n",
       "  0.004508823329825694,\n",
       "  0.0044912805368073671,\n",
       "  0.0044738921564810713,\n",
       "  0.0044567024294500084,\n",
       "  0.0044395064140026761,\n",
       "  0.0044225871316417968,\n",
       "  0.0044055275357324829,\n",
       "  0.0043888996547097733,\n",
       "  0.0043723626097976259,\n",
       "  0.0043555739359106629,\n",
       "  0.0043393059199678769,\n",
       "  0.0043230765352433457,\n",
       "  0.0043069532664352719,\n",
       "  0.0042907911051691559,\n",
       "  0.0042749159341621311,\n",
       "  0.0042591840270260837,\n",
       "  0.0042434313251204347,\n",
       "  0.0042277649601370447,\n",
       "  0.0042123828627445526,\n",
       "  0.0041970329242799623,\n",
       "  0.0041817151865786656,\n",
       "  0.0041666191077514847,\n",
       "  0.0041517562538893032,\n",
       "  0.0041367273134459816,\n",
       "  0.0041219456252210643,\n",
       "  0.0041071864549229774,\n",
       "  0.0040925952311290613,\n",
       "  0.0040780957595192759,\n",
       "  0.0040637538654115044,\n",
       "  0.0040494051380005405,\n",
       "  0.0040353053246420696,\n",
       "  0.0040210416048375415,\n",
       "  0.0040070295050512808,\n",
       "  0.0039931295908145229,\n",
       "  0.0039793053602170288,\n",
       "  0.0039654434432090579,\n",
       "  0.0039518883873943976,\n",
       "  0.0039382978299908212,\n",
       "  0.0039248758823436239,\n",
       "  0.0039115276035652548,\n",
       "  0.003898171290473893,\n",
       "  0.0038850229380743306,\n",
       "  0.0038717406013604695,\n",
       "  0.0038588352290370056,\n",
       "  0.003845812070994793,\n",
       "  0.0038329694414017466,\n",
       "  0.003820390807982214,\n",
       "  0.0038074279489339561,\n",
       "  0.0037948174948344306,\n",
       "  0.0037822349229532462,\n",
       "  0.0037697967733784183,\n",
       "  0.0037574653127323229,\n",
       "  0.0037450231777665286,\n",
       "  0.0037327976839675822,\n",
       "  0.0037206701842142646,\n",
       "  0.0037085384124571929,\n",
       "  0.0036965748652717746,\n",
       "  0.0036845685232956525,\n",
       "  0.0036727584966515089,\n",
       "  0.0036608507923757464,\n",
       "  0.0036491436457138419,\n",
       "  0.0036375489102580105,\n",
       "  0.0036259173273428947,\n",
       "  0.0036144434941537914,\n",
       "  0.003602981815958718,\n",
       "  0.0035915280771809275,\n",
       "  0.0035802243835533513,\n",
       "  0.0035689705446953538,\n",
       "  0.0035578181595956517,\n",
       "  0.0035466634663086521,\n",
       "  0.0035356077829971936,\n",
       "  0.0035246329709727282,\n",
       "  0.0035137029454976867,\n",
       "  0.0035028550594600736,\n",
       "  0.0034920796589601008,\n",
       "  0.0034813600508388108,\n",
       "  0.003470628548869694,\n",
       "  0.0034599951272657127,\n",
       "  0.0034495394331026242,\n",
       "  0.0034389430751286141,\n",
       "  0.0034285668727821769,\n",
       "  0.0034181821704439078,\n",
       "  0.0034079181899900925,\n",
       "  0.0033976425960162569,\n",
       "  0.003387411200906857,\n",
       "  0.003377257558629694,\n",
       "  0.0033671448617114087,\n",
       "  0.0033571084705391521,\n",
       "  0.0033471288516836355,\n",
       "  0.0033372669754107233,\n",
       "  0.0033273934070361028,\n",
       "  0.0033175580336503559,\n",
       "  0.0033077584324496755,\n",
       "  0.0032980932707583582,\n",
       "  0.0032884344943450126,\n",
       "  0.0032788180923386733,\n",
       "  0.0032693100679674772,\n",
       "  0.003259809149494035,\n",
       "  0.0032503538729927021,\n",
       "  0.0032409965414766286,\n",
       "  0.0032316143373702579,\n",
       "  0.0032223425385740222,\n",
       "  0.0032130938877136259,\n",
       "  0.0032039138515101706,\n",
       "  0.0031948057141590182,\n",
       "  0.0031857186932693418,\n",
       "  0.0031766686250236259,\n",
       "  0.0031676606516271599,\n",
       "  0.0031587289266098791,\n",
       "  0.0031498650823008636,\n",
       "  0.0031410475458348596,\n",
       "  0.0031321920787373623,\n",
       "  0.0031234239033336498,\n",
       "  0.0031147334741564352,\n",
       "  0.0031061030726601182],\n",
       " [])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data,monitor_evaluation_accuracy=True, monitor_training_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
